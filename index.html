<!DOCTYPE html>
<html lang="pt-BR">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conceitos de LLM</title>
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        neonPink: '#ff2d95',
                        neonCyan: '#00f0ff',
                        neonGreen: '#39ff14',
                        neonPurple: '#bf5fff',
                        neonYellow: '#ffe600',
                        neonOrange: '#ff6a00',
                        darkBg: '#0a0a0f',
                        darkCard: '#12121a',
                        darkSurface: '#1a1a2e',
                    }
                }
            }
        }
    </script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');
        body { font-family: 'Inter', sans-serif; }

        .neon-glow-pink { box-shadow: 0 0 15px rgba(255, 45, 149, 0.3), inset 0 0 15px rgba(255, 45, 149, 0.05); }
        .neon-glow-cyan { box-shadow: 0 0 15px rgba(0, 240, 255, 0.3), inset 0 0 15px rgba(0, 240, 255, 0.05); }
        .neon-glow-green { box-shadow: 0 0 15px rgba(57, 255, 20, 0.3), inset 0 0 15px rgba(57, 255, 20, 0.05); }
        .neon-glow-purple { box-shadow: 0 0 15px rgba(191, 95, 255, 0.3), inset 0 0 15px rgba(191, 95, 255, 0.05); }
        .neon-glow-yellow { box-shadow: 0 0 15px rgba(255, 230, 0, 0.3), inset 0 0 15px rgba(255, 230, 0, 0.05); }

        .neon-glow-pink:hover { box-shadow: 0 0 25px rgba(255, 45, 149, 0.5), inset 0 0 25px rgba(255, 45, 149, 0.1); }
        .neon-glow-cyan:hover { box-shadow: 0 0 25px rgba(0, 240, 255, 0.5), inset 0 0 25px rgba(0, 240, 255, 0.1); }
        .neon-glow-green:hover { box-shadow: 0 0 25px rgba(57, 255, 20, 0.5), inset 0 0 25px rgba(57, 255, 20, 0.1); }
        .neon-glow-purple:hover { box-shadow: 0 0 25px rgba(191, 95, 255, 0.5), inset 0 0 25px rgba(191, 95, 255, 0.1); }
        .neon-glow-yellow:hover { box-shadow: 0 0 25px rgba(255, 230, 0, 0.5), inset 0 0 25px rgba(255, 230, 0, 0.1); }

        .neon-text-pink { color: #ff2d95; text-shadow: 0 0 10px rgba(255, 45, 149, 0.5); }
        .neon-text-cyan { color: #00f0ff; text-shadow: 0 0 10px rgba(0, 240, 255, 0.5); }
        .neon-text-green { color: #39ff14; text-shadow: 0 0 10px rgba(57, 255, 20, 0.5); }
        .neon-text-purple { color: #bf5fff; text-shadow: 0 0 10px rgba(191, 95, 255, 0.5); }
        .neon-text-yellow { color: #ffe600; text-shadow: 0 0 10px rgba(255, 230, 0, 0.5); }

        .modal-overlay { animation: fadeIn 0.2s ease-out; }
        .modal-content { animation: slideUp 0.3s ease-out; }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }
        @keyframes slideUp {
            from { opacity: 0; transform: translateY(40px) scale(0.95); }
            to { opacity: 1; transform: translateY(0) scale(1); }
        }

        .modal-scroll::-webkit-scrollbar { width: 6px; }
        .modal-scroll::-webkit-scrollbar-track { background: rgba(255,255,255,0.05); border-radius: 3px; }
        .modal-scroll::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.2); border-radius: 3px; }
        .modal-scroll::-webkit-scrollbar-thumb:hover { background: rgba(255,255,255,0.3); }

        .flag-btn { transition: all 0.2s ease; }
        .flag-btn:hover { transform: scale(1.15); }
        .flag-btn.active { transform: scale(1.1); box-shadow: 0 0 12px rgba(0, 240, 255, 0.5); }
        .flag-btn:not(.active) { opacity: 0.45; filter: grayscale(0.5); }
    </style>
    <!-- React & ReactDOM -->
    <script type="importmap">
    {
        "imports": {
            "react": "https://esm.sh/react@18.2.0?dev",
            "react-dom/client": "https://esm.sh/react-dom@18.2.0/client?dev",
            "lucide-react": "https://esm.sh/lucide-react@0.263.1?dev"
        }
    }
    </script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
</head>

<body class="bg-darkBg text-gray-100">
    <div id="root"></div>

    <script type="text/babel" data-type="module">
        import React, { useState, useEffect } from 'react';
        import { createRoot } from 'react-dom/client';
        import { Search, Brain, Sparkles, X, ExternalLink, RefreshCw, Github, BookOpen, Lightbulb, Code, ArrowRight } from 'lucide-react';

        // ── i18n UI strings ──
        const ui = {
            pt: {
                pageTitle: "30 Conceitos de LLM",
                subtitle: "Explore os fundamentos da Inteligência Artificial Generativa.",
                tapHint: "Clique nos cartões para revelar o significado.",
                searchPlaceholder: "Buscar conceito ou definição...",
                tapToFlip: "Toque para ver a definição",
                moreDetails: "Mais Detalhes",
                howItWorks: "Como Funciona",
                examples: "Exemplos",
                keyPoints: "Pontos-Chave",
                noResults: (term) => `Nenhum conceito encontrado para "${term}"`,
                clearSearch: "Limpar busca",
                footer: "© 2026 Guia LLM Interativo.",
                categories: { foundations: "Fundamentos", training: "Treinamento", prompting: "Prompts", inference: "Inferência", application: "Aplicação" },
            },
            en: {
                pageTitle: "30 LLM Concepts",
                subtitle: "Explore the fundamentals of Generative Artificial Intelligence.",
                tapHint: "Click the cards to reveal the meaning.",
                searchPlaceholder: "Search concept or definition...",
                tapToFlip: "Tap to see definition",
                moreDetails: "More Details",
                howItWorks: "How It Works",
                examples: "Examples",
                keyPoints: "Key Points",
                noResults: (term) => `No concepts found for "${term}"`,
                clearSearch: "Clear search",
                footer: "© 2026 Interactive LLM Guide.",
                categories: { foundations: "Foundations", training: "Training", prompting: "Prompts", inference: "Inference", application: "Application" },
            },
        };

        // ── Category config ──
        const categoryConfig = {
            foundations: { neonColor: "#ff2d95", glowClass: "neon-glow-pink", textClass: "neon-text-pink", borderColor: "border-neonPink/30", dotColor: "bg-neonPink" },
            training:    { neonColor: "#ffe600", glowClass: "neon-glow-yellow", textClass: "neon-text-yellow", borderColor: "border-neonYellow/30", dotColor: "bg-neonYellow" },
            prompting:   { neonColor: "#00f0ff", glowClass: "neon-glow-cyan", textClass: "neon-text-cyan", borderColor: "border-neonCyan/30", dotColor: "bg-neonCyan" },
            inference:   { neonColor: "#bf5fff", glowClass: "neon-glow-purple", textClass: "neon-text-purple", borderColor: "border-neonPurple/30", dotColor: "bg-neonPurple" },
            application: { neonColor: "#39ff14", glowClass: "neon-glow-green", textClass: "neon-text-green", borderColor: "border-neonGreen/30", dotColor: "bg-neonGreen" },
        };

        // ── Concepts data ──
        const concepts = {
            pt: [
                // ── Fundamentos ──
                { title: "LLM", description: "Um modelo que gera texto prevendo o próximo token mais provável.", category: "foundations", details: { fullDescription: "LLM (Large Language Model) é um tipo de modelo de inteligência artificial treinado em enormes quantidades de texto para compreender e gerar linguagem natural. Esses modelos utilizam arquiteturas de redes neurais profundas, principalmente baseadas em Transformers, com bilhões de parâmetros.", howItWorks: "O modelo processa texto convertendo-o em tokens, que passam por múltiplas camadas de atenção (attention layers). Cada camada aprende diferentes aspectos da linguagem — desde gramática básica até raciocínio complexo. Na geração, o modelo prevê o próximo token mais provável dado o contexto anterior, repetindo esse processo token por token.", examples: ["GPT-4, Claude, LLaMA e Gemini são exemplos de LLMs modernos", "Um LLM pode completar frases, responder perguntas, traduzir idiomas e escrever código", "Modelos menores como Phi-3 e Mistral 7B demonstram que eficiência também importa"], keyPoints: ["Baseados na arquitetura Transformer (2017)", "Treinados em trilhões de tokens de texto", "Capacidades emergentes surgem com escala", "Podem ser especializados via fine-tuning"] } },
                { title: "Token", description: "Um pedaço de texto, como uma palavra ou símbolo de pontuação.", category: "foundations", details: { fullDescription: "Tokens são as unidades fundamentais que os LLMs processam. Em vez de trabalhar com caracteres individuais ou palavras completas, os modelos dividem o texto em pedaços otimizados chamados tokens. Um token pode ser uma palavra inteira, parte de uma palavra, um caractere ou até um símbolo de pontuação.", howItWorks: "Cada token é mapeado para um número único no vocabulário do modelo (geralmente 30.000-100.000 tokens). Palavras comuns como 'the' são um único token, enquanto palavras raras podem ser divididas em múltiplos tokens. Em português, acentos e caracteres especiais frequentemente resultam em mais tokens por palavra.", examples: ["'Olá mundo' pode ser tokenizado como ['Ol', 'á', ' mundo'] — 3 tokens", "A palavra 'inteligência' pode ser dividida em ['intel', 'ig', 'ência']", "Em inglês, 1 token ≈ 4 caracteres ou ≈ 0.75 palavras em média"], keyPoints: ["São a menor unidade de processamento do LLM", "O custo das APIs é calculado por número de tokens", "Diferentes modelos usam diferentes esquemas de tokenização", "A eficiência de tokenização varia entre idiomas"] } },
                { title: "Tokenization", description: "Processo de conversão de texto em uma sequência de tokens.", category: "foundations", details: { fullDescription: "Tokenização é o processo de converter texto bruto em uma sequência de tokens que o modelo pode processar. É o primeiro passo em qualquer pipeline de LLM e afeta diretamente a eficiência e capacidade do modelo. Algoritmos como BPE (Byte Pair Encoding) e WordPiece são usados para criar vocabulários otimizados.", howItWorks: "O algoritmo BPE começa com caracteres individuais e iterativamente mescla os pares mais frequentes para formar tokens maiores. Isso cria um equilíbrio entre ter tokens pequenos (flexíveis mas ineficientes) e tokens grandes (eficientes mas rígidos). O resultado é um vocabulário que captura palavras comuns inteiras e divide palavras raras em sub-palavras reconhecíveis.", examples: ["BPE: 'unhappiness' → ['un', 'happiness'] ou ['un', 'happ', 'iness']", "O tokenizador do GPT-4 (cl100k_base) tem ~100.000 tokens no vocabulário", "Emojis e caracteres Unicode especiais geralmente consomem múltiplos tokens"], keyPoints: ["BPE (Byte Pair Encoding) é o algoritmo mais usado", "O vocabulário é fixo após o treinamento", "Idiomas não-ingleses geralmente requerem mais tokens", "A qualidade da tokenização impacta o desempenho do modelo"] } },
                { title: "Embeddings", description: "Vetores numéricos que representam o significado dos tokens.", category: "foundations", details: { fullDescription: "Embeddings são representações vetoriais densas que capturam o significado semântico de tokens, palavras, frases ou documentos inteiros. Em vez de tratar palavras como símbolos discretos, embeddings as posicionam em um espaço matemático contínuo onde a proximidade reflete similaridade de significado.", howItWorks: "Cada token é convertido em um vetor de alta dimensão (ex: 768 ou 1536 dimensões). Durante o treinamento, o modelo ajusta esses vetores para que tokens com significados similares fiquem próximos no espaço vetorial. Operações matemáticas nos vetores podem revelar relações semânticas: por exemplo, vetor('rei') - vetor('homem') + vetor('mulher') ≈ vetor('rainha').", examples: ["'gato' e 'felino' terão embeddings muito próximos no espaço vetorial", "Embeddings de 1536 dimensões são usados pelo modelo text-embedding-ada-002", "Busca semântica usa similaridade de cosseno entre embeddings para encontrar textos relevantes"], keyPoints: ["Capturam relações semânticas entre conceitos", "São a base para busca semântica e RAG", "Dimensionalidade típica: 384 a 4096 dimensões", "Podem ser pré-computados e armazenados em bancos vetoriais"] } },
                { title: "Latent Space", description: "Espaço matemático onde os embeddings são organizados por significado.", category: "foundations", details: { fullDescription: "O Espaço Latente é o espaço matemático multidimensional onde as representações internas do modelo existem. Neste espaço, conceitos abstratos são organizados de forma que relações semânticas se traduzem em relações geométricas. É 'latente' porque não é diretamente observável — existe apenas nas camadas internas do modelo.", howItWorks: "À medida que o texto passa pelas camadas do modelo, ele é transformado em representações cada vez mais abstratas neste espaço. As primeiras camadas capturam características sintáticas (gramática, estrutura), enquanto camadas mais profundas capturam semântica (significado, contexto, intenção). O espaço latente permite que o modelo generalize e faça analogias.", examples: ["Países e suas capitais formam clusters paralelos no espaço latente", "Palavras em diferentes idiomas com mesmo significado ficam próximas", "O modelo pode interpolar entre conceitos para gerar conteúdo criativo"], keyPoints: ["Organiza conhecimento de forma contínua e navegável", "Permite generalização para dados nunca vistos", "A qualidade do espaço latente determina a capacidade do modelo", "Técnicas como t-SNE permitem visualizar partes deste espaço em 2D/3D"] } },
                { title: "Parameters", description: "Variáveis internas que armazenam os padrões aprendidos pelo modelo.", category: "foundations", details: { fullDescription: "Parâmetros são os valores numéricos (pesos e vieses) dentro da rede neural que são ajustados durante o treinamento para capturar padrões na linguagem. O número de parâmetros é frequentemente usado como medida do tamanho e capacidade do modelo — modelos maiores geralmente demonstram capacidades mais sofisticadas.", howItWorks: "Cada conexão entre neurônios na rede tem um peso associado. Durante o treinamento, esses pesos são ajustados através de backpropagation para minimizar o erro de previsão. Em um Transformer, os parâmetros incluem as matrizes de atenção (Query, Key, Value), as camadas feed-forward, e os embeddings de entrada/saída.", examples: ["GPT-3 tem 175 bilhões de parâmetros", "LLaMA 2 vem em versões de 7B, 13B e 70B parâmetros", "Claude e GPT-4 têm quantidade de parâmetros não divulgada publicamente"], keyPoints: ["Mais parâmetros geralmente = mais capacidade, mas maior custo", "Modelos modernos variam de 1B a mais de 1T de parâmetros", "Quantização pode reduzir o tamanho sem grande perda de qualidade", "A lei de escala (scaling law) relaciona parâmetros, dados e desempenho"] } },
                // ── Treinamento ──
                { title: "Pre-training", description: "Treinamento em dados de texto massivos para aprender padrões de linguagem.", category: "training", details: { fullDescription: "Pré-treinamento é a fase inicial e mais custosa do desenvolvimento de um LLM, onde o modelo aprende os fundamentos da linguagem a partir de enormes quantidades de texto. Nesta fase, o modelo desenvolve uma compreensão ampla de gramática, fatos, raciocínio e até algum senso comum.", howItWorks: "O modelo é treinado com o objetivo de prever o próximo token em sequências de texto. Bilhões de textos da internet, livros, artigos e código são processados. O treinamento pode levar semanas ou meses em clusters de milhares de GPUs, custando milhões de dólares. O resultado é um modelo base que pode completar texto mas ainda não segue instruções.", examples: ["O treinamento do GPT-4 custou estimados $100 milhões em computação", "Common Crawl, Wikipedia e repositórios GitHub são fontes comuns de dados", "O LLaMA foi treinado em 1.4 trilhões de tokens"], keyPoints: ["É a fase mais cara e demorada (semanas/meses)", "Usa aprendizado auto-supervisionado (next-token prediction)", "A qualidade e diversidade dos dados são cruciais", "Define as capacidades fundamentais do modelo"] } },
                { title: "Base Model", description: "Um modelo pré-treinado que prevê texto, mas não segue instruções.", category: "training", details: { fullDescription: "Um Modelo Base (ou Foundation Model) é o resultado direto do pré-treinamento. Ele aprendeu padrões estatísticos da linguagem e pode completar texto de forma coerente, mas não foi treinado para seguir instruções ou manter conversas. Ele simplesmente continua o texto que recebe da forma mais provável.", howItWorks: "Quando você fornece texto a um modelo base, ele gera a continuação mais estatisticamente provável. Se você escrever uma pergunta, em vez de respondê-la, ele pode gerar mais perguntas similares ou continuar como se fosse parte de um documento. É como um autocomplete muito sofisticado que entende contexto mas não entende intenção.", examples: ["Input: 'A capital do Brasil é' → Output provável: 'Brasília, cidade planejada...'", "Input: 'Qual é a capital do Brasil?' → Pode gerar: 'Qual é a capital da Argentina?' (completando um quiz)", "GPT-3 base, LLaMA base e Mistral base são exemplos de modelos base"], keyPoints: ["Excelente em completar texto, mas imprevisível para conversas", "Serve como fundação para fine-tuning e alinhamento", "Ainda é útil para tarefas como few-shot learning", "Pesquisadores frequentemente preferem modelos base para experimentação"] } },
                { title: "Instruct Model", description: "Um modelo base treinado adicionalmente para seguir instruções e responder de forma útil.", category: "training", details: { fullDescription: "Um Modelo Instruct é um modelo base que passou por treinamento adicional para entender e seguir instruções em linguagem natural. Este treinamento transforma o modelo de um simples completador de texto em um assistente capaz de responder perguntas, seguir comandos e manter conversas produtivas.", howItWorks: "O processo envolve fine-tuning supervisionado com pares de instrução-resposta criados por humanos, seguido de RLHF (Reinforcement Learning from Human Feedback). Humanos escrevem exemplos de como o modelo deveria responder a diferentes tipos de instruções, e o modelo aprende a replicar esse padrão de comportamento.", examples: ["GPT-4-turbo, Claude 3 Opus e Gemini Pro são modelos instruct", "'Resuma este texto em 3 pontos' → o modelo entende e executa a tarefa", "ChatGPT é essencialmente o GPT base transformado em modelo instruct via RLHF"], keyPoints: ["Segue instruções de forma confiável e previsível", "Mantém conversas coerentes e contextuais", "Pode recusar pedidos inapropriados (safety training)", "É o tipo de modelo usado em produtos como ChatGPT e Claude"] } },
                { title: "Fine-Tuning", description: "Treinamento adicional em um conjunto de dados menor para moldar o comportamento do modelo.", category: "training", details: { fullDescription: "Fine-tuning é o processo de continuar o treinamento de um modelo pré-treinado usando um conjunto de dados menor e especializado. Isso permite adaptar o modelo para tarefas específicas, domínios de conhecimento ou estilos de resposta sem precisar treinar um modelo do zero.", howItWorks: "O modelo base tem seus pesos levemente ajustados usando dados específicos do domínio. Técnicas como LoRA (Low-Rank Adaptation) permitem fazer fine-tuning eficiente modificando apenas uma pequena fração dos parâmetros. O processo geralmente requer centenas a milhares de exemplos de alta qualidade e pode ser feito em horas com uma única GPU.", examples: ["Fine-tuning para atendimento ao cliente com histórico de conversas reais", "Adaptar um modelo para gerar código em uma linguagem específica", "LoRA permite fine-tuning de modelos de 7B parâmetros em uma GPU de 16GB"], keyPoints: ["Muito mais barato e rápido que pré-treinamento", "LoRA e QLoRA tornaram fine-tuning acessível", "Risco de catastrophic forgetting (esquecer conhecimento anterior)", "A qualidade dos dados é mais importante que a quantidade"] } },
                { title: "Alignment", description: "Garantir que o comportamento do modelo seja útil, honesto e inofensivo.", category: "training", details: { fullDescription: "Alinhamento é o processo de garantir que um modelo de IA se comporte de acordo com valores e intenções humanas. Isso inclui ser útil (helpful), honesto (honest) e inofensivo (harmless) — os chamados critérios HHH. É considerado um dos maiores desafios da IA moderna.", howItWorks: "O alinhamento combina várias técnicas: fine-tuning supervisionado com exemplos de comportamento desejado, RLHF para otimizar preferências humanas, Constitutional AI (CAI) onde o modelo aprende a seguir princípios éticos, e red-teaming para identificar falhas. O objetivo é criar modelos que entendam não apenas O QUE fazer, mas também O QUE NÃO fazer.", examples: ["Um modelo alinhado recusa gerar instruções para atividades ilegais", "Constitutional AI da Anthropic usa princípios escritos para guiar o comportamento", "O modelo admite incerteza em vez de fabricar informações (honestidade)"], keyPoints: ["Baseado nos critérios HHH: Helpful, Honest, Harmless", "É um problema em aberto na pesquisa de IA", "Inclui safety training contra usos maliciosos", "O desalinhamento é um risco existencial segundo alguns pesquisadores"] } },
                { title: "RLHF", description: "Uso de respostas classificadas por humanos para guiar o comportamento do modelo.", category: "training", details: { fullDescription: "RLHF (Reinforcement Learning from Human Feedback) é uma técnica de treinamento onde avaliadores humanos classificam diferentes respostas do modelo, e essas classificações são usadas para treinar um modelo de recompensa que guia o aprendizado por reforço. Foi a técnica chave que tornou o ChatGPT tão eficaz.", howItWorks: "O processo tem 3 etapas: (1) Fine-tuning supervisionado com exemplos humanos, (2) Treinamento de um modelo de recompensa — humanos comparam pares de respostas e escolhem a melhor, criando um dataset de preferências, (3) Otimização por PPO (Proximal Policy Optimization) usando o modelo de recompensa para guiar o LLM a gerar respostas preferidas pelos humanos.", examples: ["Avaliadores comparam duas respostas e escolhem a mais útil e segura", "O ChatGPT original usou RLHF extensivamente para melhorar o GPT-3.5", "DPO (Direct Preference Optimization) é uma alternativa mais simples ao RLHF"], keyPoints: ["Responsável pela revolução de usabilidade do ChatGPT", "Requer trabalho humano significativo para avaliação", "PPO é o algoritmo de RL mais usado neste contexto", "DPO e RLAIF são alternativas mais recentes e eficientes"] } },
                // ── Prompts ──
                { title: "Prompt", description: "Entrada completa enviada ao modelo, incluindo instruções e contexto.", category: "prompting", details: { fullDescription: "Um prompt é todo o texto de entrada enviado ao modelo, incluindo instruções, contexto, exemplos e a pergunta ou tarefa em si. A qualidade do prompt tem impacto direto na qualidade da resposta — a arte e ciência de criar bons prompts é chamada de 'Prompt Engineering'.", howItWorks: "O modelo processa o prompt inteiro como contexto para gerar sua resposta. Cada token do prompt influencia a distribuição de probabilidade dos tokens de saída. Prompts bem estruturados guiam o modelo para o formato, estilo e conteúdo desejados. A ordem das informações, a clareza das instruções e a especificidade dos requisitos são fatores cruciais.", examples: ["Prompt simples: 'O que é machine learning?'", "Prompt estruturado: 'Você é um professor. Explique machine learning para um iniciante em 3 parágrafos.'", "Prompt com contexto: 'Dado o artigo abaixo, extraia os 5 pontos principais: [artigo]'"], keyPoints: ["Prompt Engineering é uma habilidade valiosa e em crescimento", "Prompts claros e específicos geram melhores resultados", "O formato do prompt afeta formato da resposta", "Técnicas como Chain-of-Thought melhoram raciocínio via prompt"] } },
                { title: "System Prompt", description: "Instruções de alto nível que definem o papel e os limites do modelo.", category: "prompting", details: { fullDescription: "O System Prompt é uma instrução especial que define o comportamento, personalidade, limitações e contexto do modelo antes da interação com o usuário. Ele funciona como uma 'configuração' do assistente, estabelecendo regras que devem ser seguidas em toda a conversa.", howItWorks: "O system prompt é processado como o primeiro bloco de contexto na conversa. Ele tem prioridade especial na maioria das implementações e influencia todas as respostas subsequentes. APIs como a da OpenAI e Anthropic têm um campo dedicado para system prompts, separado das mensagens do usuário.", examples: ["'Você é um assistente especializado em direito brasileiro. Sempre cite as leis relevantes.'", "'Responda apenas em JSON. Nunca inclua explicações em texto.'", "'Você é um tutor paciente. Use linguagem simples e dê exemplos práticos.'"], keyPoints: ["Define persona, tom e limites do assistente", "Separado do user prompt na maioria das APIs", "Pode incluir instruções de segurança e restrições", "Não é 100% inviolável — jailbreaks podem contorná-lo"] } },
                { title: "User Prompt", description: "Pergunta ou instrução específica fornecida pelo usuário.", category: "prompting", details: { fullDescription: "O User Prompt é a mensagem direta do usuário ao modelo — a pergunta, comando ou tarefa que o usuário deseja que o modelo execute. É a parte mais dinâmica da interação e varia a cada turno da conversa.", howItWorks: "O user prompt é combinado com o system prompt e o histórico da conversa para formar o contexto completo. O modelo gera sua resposta considerando todas essas informações. Em aplicações multi-turno, cada novo user prompt é adicionado ao histórico, permitindo conversas contextuais.", examples: ["'Explique a teoria da relatividade em termos simples'", "'Corrija os erros neste código Python: [código]'", "'Continue a história que começamos no turno anterior'"], keyPoints: ["É a interface direta entre humano e modelo", "Combinado com system prompt para contexto completo", "Histórico de conversas anteriores influencia a resposta", "Clareza e especificidade melhoram a qualidade da resposta"] } },
                { title: "Context Window", description: "Número máximo de tokens que o modelo pode processar de uma vez.", category: "prompting", details: { fullDescription: "A Context Window (Janela de Contexto) é o número máximo de tokens que um modelo pode considerar simultaneamente, incluindo tanto a entrada (prompt) quanto a saída (resposta). É uma limitação fundamental que determina quanta informação o modelo pode processar de uma vez.", howItWorks: "O mecanismo de atenção do Transformer calcula relações entre todos os pares de tokens na janela, resultando em complexidade quadrática O(n²). Isso limita naturalmente o tamanho da janela. Técnicas como atenção esparsa, RoPE e ALiBi permitem expandir a janela sem custo quadrático completo.", examples: ["GPT-3.5: 4K-16K tokens | GPT-4: 8K-128K tokens", "Claude 3: até 200K tokens (~150.000 palavras ou ~500 páginas)", "Gemini 1.5 Pro: até 1M tokens (equivalente a vários livros)"], keyPoints: ["Inclui tanto input quanto output no limite total", "Janelas maiores = mais contexto mas maior custo e latência", "Técnicas como RAG ajudam a contornar limitações de contexto", "A capacidade de usar efetivamente toda a janela varia entre modelos"] } },
                { title: "Zero-Shot Learning", description: "Realização de uma tarefa sem exemplos no prompt.", category: "prompting", details: { fullDescription: "Zero-Shot Learning é a capacidade do modelo de realizar uma tarefa apenas com uma instrução, sem receber nenhum exemplo de como a tarefa deve ser executada. Isso é possível porque o modelo internalizou padrões suficientes durante o pré-treinamento para generalizar para novas tarefas.", howItWorks: "O modelo usa seu conhecimento internalizado durante o pré-treinamento para entender a instrução e gerar uma resposta apropriada. A capacidade zero-shot melhora significativamente com o tamanho do modelo — modelos maiores demonstram capacidades zero-shot que modelos menores não possuem (capacidades emergentes).", examples: ["'Classifique este texto como positivo ou negativo: O filme foi incrível!' → 'Positivo'", "'Traduza para inglês: Bom dia' → 'Good morning'", "'Resuma este artigo em 3 pontos:' — sem exemplos de resumos anteriores"], keyPoints: ["Demonstra a generalização do conhecimento do modelo", "Funciona melhor em modelos maiores e mais capazes", "Ideal para tarefas comuns que não requerem formato específico", "Pode ser insuficiente para tarefas muito especializadas ou com formato rígido"] } },
                { title: "Few-Shot Learning", description: "Inclusão de exemplos no prompt para guiar o formato ou comportamento da saída.", category: "prompting", details: { fullDescription: "Few-Shot Learning é a técnica de incluir alguns exemplos (geralmente 2-5) no prompt para demonstrar ao modelo exatamente como a tarefa deve ser executada. Os exemplos servem como 'template' que o modelo segue para gerar respostas no formato e estilo desejados.", howItWorks: "Os exemplos criam um padrão in-context que o modelo reconhece e replica. Não há atualização de pesos — o modelo simplesmente usa os exemplos como contexto para inferência. A eficácia depende da qualidade e representatividade dos exemplos escolhidos. Exemplos diversos e bem escolhidos produzem melhores resultados.", examples: ["Sentimento: 'Amo este produto!' → Positivo | 'Péssimo serviço' → Negativo | 'O atendimento foi rápido' → ?", "Fornecendo 3 exemplos de formatação JSON antes de pedir a conversão de dados", "Mostrando exemplos de tom formal antes de pedir a redação de um e-mail corporativo"], keyPoints: ["2-5 exemplos geralmente são suficientes", "Exemplos diversos melhoram a generalização", "Consome tokens da context window (trade-off)", "Mais eficaz que zero-shot para tarefas com formato específico"] } },
                // ── Inferência ──
                { title: "Chain of Thought", description: "Solicitar ao modelo que mostre o raciocínio passo a passo.", category: "inference", details: { fullDescription: "Chain of Thought (CoT) é uma técnica de prompting que instrui o modelo a 'pensar em voz alta', decompondo problemas complexos em passos intermediários antes de chegar à resposta final. Isso melhora dramaticamente o desempenho em tarefas de raciocínio, matemática e lógica.", howItWorks: "Ao gerar tokens intermediários de raciocínio, o modelo efetivamente 'usa' esses tokens como memória de trabalho adicional. Cada passo intermediário condiciona os passos seguintes, permitindo raciocínio mais preciso. Variações incluem CoT zero-shot ('Pense passo a passo') e CoT few-shot (com exemplos de raciocínio).", examples: ["'Pense passo a passo: Se João tem 3 maçãs e Maria tem o dobro, quantas elas têm juntos?'", "Modelo raciocina: '1) João tem 3 maçãs. 2) Maria tem 3×2=6 maçãs. 3) Total: 3+6=9'", "Tree of Thought expande CoT com múltiplos caminhos de raciocínio paralelos"], keyPoints: ["Melhora significativamente o desempenho em tarefas de raciocínio", "Basta adicionar 'Pense passo a passo' ao prompt (zero-shot CoT)", "Consome mais tokens mas produz respostas mais confiáveis", "Base para modelos de raciocínio como o1 e DeepSeek-R1"] } },
                { title: "Inference", description: "Processo de geração de tokens de saída a partir de um modelo treinado.", category: "inference", details: { fullDescription: "Inferência é o processo de usar um modelo já treinado para gerar respostas. Diferente do treinamento (que ajusta pesos), a inferência mantém os pesos fixos e apenas calcula os tokens de saída. É o que acontece cada vez que você envia uma mensagem ao ChatGPT ou Claude.", howItWorks: "O prompt é tokenizado e passado pelas camadas do modelo. Na última camada, o modelo gera uma distribuição de probabilidade sobre todo o vocabulário para o próximo token. Um token é selecionado (usando temperatura e outros parâmetros), adicionado à sequência, e o processo se repete autorregressivamente até gerar um token de parada ou atingir o limite.", examples: ["Cada mensagem enviada ao ChatGPT inicia um processo de inferência", "Inferência em batch: processar múltiplas requisições simultaneamente para eficiência", "Speculative decoding usa um modelo menor para acelerar a inferência do modelo grande"], keyPoints: ["Processo autorregressivo: um token por vez, sequencialmente", "Muito mais barato que treinamento, mas ainda significativo em escala", "KV-cache otimiza re-computações durante a geração", "Técnicas como quantização reduzem custo de inferência"] } },
                { title: "Latency", description: "Tempo entre o envio de um prompt e o recebimento da saída.", category: "inference", details: { fullDescription: "Latência é o tempo de resposta do modelo, medido desde o envio do prompt até o recebimento da resposta completa (ou do primeiro token, no caso de streaming). É um fator crítico para a experiência do usuário e para aplicações em tempo real.", howItWorks: "A latência total inclui: tempo de rede, tokenização do input, processamento do prompt (prefill), e geração autorregressiva de cada token (decode). O prefill processa todos os tokens de entrada em paralelo e é proporcional ao tamanho do prompt. O decode é sequencial e proporcional ao tamanho da resposta. Streaming mostra tokens conforme são gerados, reduzindo a latência percebida.", examples: ["Time to First Token (TTFT): tipicamente 0.5-2 segundos para modelos grandes", "Streaming reduz a latência percebida mostrando tokens conforme são gerados", "Modelos menores (7B) podem ter TTFT de menos de 100ms em hardware adequado"], keyPoints: ["TTFT (Time to First Token) é a métrica mais importante para UX", "Prompts maiores aumentam a latência de prefill", "Modelos menores são significativamente mais rápidos", "Edge deployment e quantização ajudam a reduzir latência"] } },
                { title: "Temperature", description: "Um parâmetro que controla a aleatoriedade na seleção de tokens.", category: "inference", details: { fullDescription: "Temperature é um hiperparâmetro que controla a aleatoriedade/criatividade na geração de texto. Valores baixos (0-0.3) tornam o modelo mais determinístico e focado, enquanto valores altos (0.7-1.5) tornam as respostas mais diversas e criativas, mas potencialmente menos coerentes.", howItWorks: "A temperature modifica a distribuição de probabilidade dos tokens antes da amostragem. Matematicamente, as logits (scores brutos) são divididas pela temperature antes do softmax. Temperature 0 seleciona sempre o token mais provável (greedy). Temperature 1 mantém a distribuição original. Temperature > 1 'achata' a distribuição, dando mais chance a tokens menos prováveis.", examples: ["Temperature 0: 'A capital do Brasil é Brasília.' (sempre igual)", "Temperature 0.7: respostas variadas mas coerentes — ideal para escrita criativa", "Temperature 1.5: pode gerar texto surpreendente mas por vezes incoerente"], keyPoints: ["0 = determinístico, 1 = distribuição original, >1 = mais aleatório", "Use baixa (0-0.3) para tarefas factuais e código", "Use média (0.5-0.8) para escrita criativa equilibrada", "Top-p (nucleus sampling) é outro parâmetro complementar"] } },
                { title: "Hallucination", description: "Geração confiante de informações incorretas ou fabricadas.", category: "inference", details: { fullDescription: "Alucinação ocorre quando o modelo gera informações que parecem corretas e são apresentadas com confiança, mas são factualmente incorretas, inventadas ou não fundamentadas. É um dos maiores desafios dos LLMs atuais, pois o modelo não distingue entre 'lembrar' fatos reais e 'inventar' fatos plausíveis.", howItWorks: "LLMs são treinados para gerar texto estatisticamente plausível, não necessariamente verdadeiro. O modelo aprende padrões de como fatos são expressos, mas não tem um 'banco de dados' verificável. Quando o modelo não 'sabe' algo, ele pode gerar uma resposta que segue o padrão estatístico correto mas contém informações fabricadas. Isso ocorre porque o modelo otimiza plausibilidade, não veracidade.", examples: ["Citar artigos acadêmicos com títulos, autores e DOIs completamente inventados", "Afirmar com confiança que um evento histórico ocorreu em uma data incorreta", "Inventar funcionalidades inexistentes de bibliotecas de programação"], keyPoints: ["O modelo não sabe que está 'inventando' — gera texto plausível", "Técnicas como RAG e Grounding ajudam a mitigar alucinações", "Verificação humana continua sendo essencial para conteúdo crítico", "Modelos mais recentes alucinam menos, mas o problema persiste"] } },
                { title: "Grounding", description: "Restrição das saídas a informações fornecidas ou verificáveis.", category: "inference", details: { fullDescription: "Grounding é o processo de ancorar as respostas do modelo em fontes de informação verificáveis e confiáveis, em vez de depender apenas do conhecimento internalizado durante o treinamento. É a principal estratégia para combater alucinações e garantir precisão factual.", howItWorks: "O modelo recebe documentos, dados ou contexto verificável junto com a instrução, e é orientado a basear suas respostas exclusivamente nessas fontes. Técnicas incluem RAG (buscar informações relevantes antes de responder), citação de fontes, e instruções explícitas como 'responda apenas com base no contexto fornecido'.", examples: ["'Com base APENAS no documento abaixo, responda:' — grounding explícito", "RAG busca documentos relevantes e os inclui no prompt como contexto", "Google Search Grounding: modelo consulta a web antes de responder"], keyPoints: ["Principal defesa contra alucinações", "RAG é a técnica de grounding mais popular", "Pode incluir citações e referências para verificação", "Trade-off: limita criatividade mas aumenta confiabilidade"] } },
                // ── Aplicação ──
                { title: "RAG", description: "Recuperação de dados externos e adição ao prompt antes da geração.", category: "application", details: { fullDescription: "RAG (Retrieval-Augmented Generation) combina busca de informação com geração de texto. Em vez de depender apenas do conhecimento do modelo, RAG primeiro busca documentos relevantes em uma base de dados e os inclui no prompt, permitindo respostas atualizadas, precisas e citáveis.", howItWorks: "O processo tem 3 etapas: (1) Indexação: documentos são convertidos em embeddings e armazenados em um banco vetorial. (2) Recuperação: a pergunta do usuário é convertida em embedding e os documentos mais similares são recuperados via busca por similaridade. (3) Geração: os documentos recuperados são incluídos no prompt junto com a pergunta, e o LLM gera uma resposta baseada nesse contexto.", examples: ["Chatbot corporativo que busca em manuais internos antes de responder", "Assistente jurídico que consulta legislação atualizada via RAG", "Ferramentas como Perplexity AI usam RAG com busca na web em tempo real"], keyPoints: ["Resolve o problema de conhecimento desatualizado do modelo", "Reduz alucinações ao fornecer fontes verificáveis", "Bancos vetoriais como Pinecone, Weaviate e ChromaDB são usados", "A qualidade da recuperação determina a qualidade da resposta"] } },
                { title: "Workflow", description: "Uma sequência fixa e predefinida onde o LLM segue passos estabelecidos.", category: "application", details: { fullDescription: "Um Workflow de LLM é uma sequência predefinida e determinística de etapas onde o modelo é usado como componente em pontos específicos. Diferente de um agente autônomo, o fluxo de execução é fixo e controlado pelo código, não pelo modelo. Cada etapa tem entrada e saída definidas.", howItWorks: "O desenvolvedor define uma pipeline com etapas sequenciais ou paralelas. Em cada etapa, o LLM pode ser chamado para uma tarefa específica (classificar, extrair, gerar, etc). A lógica de controle (condicionais, loops, roteamento) é implementada em código, não delegada ao modelo. Frameworks como LangChain e LlamaIndex facilitam a criação de workflows.", examples: ["Pipeline de análise de e-mail: classificar → extrair entidades → gerar resposta → revisar", "Geração de relatório: coletar dados → analisar → gerar texto → formatar", "Processamento de documento: OCR → chunking → embedding → indexação"], keyPoints: ["Fluxo previsível e debugável (vs. agentes autônomos)", "Cada etapa pode usar um prompt diferente otimizado", "Mais fácil de testar e monitorar que agentes", "Ideal quando a sequência de passos é conhecida antecipadamente"] } },
                { title: "Agent", description: "Um sistema onde o LLM planeja ações, depois escolhe dinamicamente passos e ferramentas.", category: "application", details: { fullDescription: "Um Agente de IA é um sistema onde o LLM atua como o 'cérebro' que autonomamente planeja, toma decisões e executa ações usando ferramentas externas. Diferente de workflows fixos, agentes determinam dinamicamente quais passos tomar com base no contexto e nos resultados intermediários.", howItWorks: "O agente segue um loop: (1) Observar o estado atual e o objetivo, (2) Raciocinar sobre o próximo passo (usando CoT), (3) Selecionar e executar uma ferramenta (busca, código, API, etc.), (4) Observar o resultado, (5) Decidir se o objetivo foi atingido ou se mais ações são necessárias. Frameworks como AutoGPT, CrewAI e o próprio Claude com tool use implementam este padrão.", examples: ["Claude Code: agente que lê código, planeja mudanças e executa comandos", "Agente de pesquisa que busca na web, lê artigos e sintetiza informações", "Agente de atendimento que consulta banco de dados, processa pedidos e escala para humanos"], keyPoints: ["Autonomia: decide quais ações tomar dinamicamente", "Usa ferramentas (tool use) para interagir com o mundo", "Mais flexível que workflows, mas menos previsível", "Requer guardrails robustas para evitar ações indesejadas"] } },
                { title: "Multimodality", description: "Capacidade de processar múltiplos tipos de entrada, como texto e imagens.", category: "application", details: { fullDescription: "Multimodalidade é a capacidade de um modelo processar e/ou gerar diferentes tipos de dados — texto, imagens, áudio, vídeo e código. Modelos multimodais podem entender o contexto através de múltiplas modalidades simultaneamente, aproximando-se da percepção humana.", howItWorks: "Modelos multimodais usam encoders especializados para cada tipo de entrada (ex: Vision Transformer para imagens) que convertem dados de diferentes modalidades para o mesmo espaço de embedding. O modelo pode então raciocinar sobre todas as modalidades simultaneamente. Alguns modelos são multimodais na entrada (entendem imagens) mas unimodais na saída (geram apenas texto).", examples: ["GPT-4V e Claude 3 podem analisar imagens e responder perguntas sobre elas", "Gemini pode processar texto, imagens, áudio e vídeo simultaneamente", "DALL-E e Midjourney geram imagens a partir de descrições textuais"], keyPoints: ["Texto + imagem é a combinação multimodal mais comum", "Modelos recentes estão adicionando áudio e vídeo", "Permite aplicações como análise de documentos, OCR inteligente e acessibilidade", "A qualidade varia significativamente entre modalidades e modelos"] } },
                { title: "Benchmarks", description: "Testes padronizados usados para comparar as capacidades do modelo.", category: "application", details: { fullDescription: "Benchmarks são conjuntos de testes padronizados usados para medir e comparar o desempenho de diferentes modelos de IA em tarefas específicas. Eles incluem questões de raciocínio, matemática, programação, conhecimento geral e mais, fornecendo métricas objetivas de comparação.", howItWorks: "Cada benchmark consiste em um conjunto de problemas com respostas verificáveis. Os modelos são avaliados sem acesso às respostas, e sua pontuação é calculada com base na precisão. Benchmarks são cuidadosamente projetados para testar capacidades específicas e minimizar a possibilidade de memorização (data contamination).", examples: ["MMLU: 57 disciplinas acadêmicas — testa conhecimento geral amplo", "HumanEval: problemas de programação — testa capacidade de codificação", "GSM8K: problemas matemáticos de nível escolar — testa raciocínio quantitativo"], keyPoints: ["Essenciais para comparação objetiva entre modelos", "Data contamination é um risco (modelo memoriza perguntas do benchmark)", "Nenhum benchmark captura completamente a utilidade real de um modelo", "Leaderboards como LMSYS Chatbot Arena usam avaliação humana"] } },
                { title: "Guardrails", description: "Sistemas que bloqueiam entradas e saídas inseguras ou inapropriadas.", category: "application", details: { fullDescription: "Guardrails são sistemas de segurança que filtram, validam e controlam as entradas e saídas de LLMs para prevenir usos indevidos, conteúdo prejudicial e comportamentos indesejados. Funcionam como camadas de proteção ao redor do modelo, garantindo que ele opere dentro de limites aceitáveis.", howItWorks: "Guardrails operam em múltiplas camadas: (1) Filtros de entrada — detectam e bloqueiam prompts maliciosos, jailbreaks e conteúdo proibido antes de chegar ao modelo. (2) Regras de saída — verificam a resposta do modelo contra políticas definidas (PII, toxicidade, temas proibidos). (3) Validação estrutural — garantem que a saída esteja no formato correto (JSON, comprimento, etc.).", examples: ["Bloquear geração de código malicioso ou instruções perigosas", "Detectar e redatar informações pessoais (PII) nas respostas", "NeMo Guardrails da NVIDIA permite definir regras conversacionais declarativas"], keyPoints: ["Essenciais para deploy em produção de aplicações com LLM", "Incluem proteção contra prompt injection e jailbreaks", "Frameworks como Guardrails AI e NeMo facilitam a implementação", "Devem ser testadas continuamente com red-teaming"] } },
            ],
            en: [
                // ── Foundations ──
                { title: "LLM", description: "A model that generates text by predicting the most likely next token.", category: "foundations", details: { fullDescription: "LLM (Large Language Model) is a type of artificial intelligence model trained on massive amounts of text to understand and generate natural language. These models use deep neural network architectures, primarily based on Transformers, with billions of parameters.", howItWorks: "The model processes text by converting it into tokens, which pass through multiple attention layers. Each layer learns different aspects of language — from basic grammar to complex reasoning. During generation, the model predicts the most likely next token given the previous context, repeating this process token by token.", examples: ["GPT-4, Claude, LLaMA, and Gemini are examples of modern LLMs", "An LLM can complete sentences, answer questions, translate languages, and write code", "Smaller models like Phi-3 and Mistral 7B demonstrate that efficiency also matters"], keyPoints: ["Based on the Transformer architecture (2017)", "Trained on trillions of text tokens", "Emergent capabilities arise with scale", "Can be specialized through fine-tuning"] } },
                { title: "Token", description: "A piece of text, such as a word or punctuation mark.", category: "foundations", details: { fullDescription: "Tokens are the fundamental units that LLMs process. Instead of working with individual characters or complete words, models split text into optimized chunks called tokens. A token can be a whole word, part of a word, a character, or even a punctuation mark.", howItWorks: "Each token is mapped to a unique number in the model's vocabulary (typically 30,000-100,000 tokens). Common words like 'the' are a single token, while rare words may be split into multiple tokens. In languages with special characters, accents often result in more tokens per word.", examples: ["'Hello world' is typically tokenized as ['Hello', ' world'] — 2 tokens", "The word 'unbelievable' might be split into ['un', 'believ', 'able']", "In English, 1 token ≈ 4 characters or ≈ 0.75 words on average"], keyPoints: ["The smallest processing unit of an LLM", "API costs are calculated by token count", "Different models use different tokenization schemes", "Tokenization efficiency varies across languages"] } },
                { title: "Tokenization", description: "The process of converting text into a sequence of tokens.", category: "foundations", details: { fullDescription: "Tokenization is the process of converting raw text into a sequence of tokens that the model can process. It is the first step in any LLM pipeline and directly affects the model's efficiency and capability. Algorithms like BPE (Byte Pair Encoding) and WordPiece are used to create optimized vocabularies.", howItWorks: "The BPE algorithm starts with individual characters and iteratively merges the most frequent pairs to form larger tokens. This creates a balance between having small tokens (flexible but inefficient) and large tokens (efficient but rigid). The result is a vocabulary that captures common whole words and splits rare words into recognizable sub-words.", examples: ["BPE: 'unhappiness' → ['un', 'happiness'] or ['un', 'happ', 'iness']", "GPT-4's tokenizer (cl100k_base) has ~100,000 tokens in its vocabulary", "Emojis and special Unicode characters typically consume multiple tokens"], keyPoints: ["BPE (Byte Pair Encoding) is the most widely used algorithm", "The vocabulary is fixed after training", "Non-English languages generally require more tokens", "Tokenization quality impacts model performance"] } },
                { title: "Embeddings", description: "Numerical vectors that represent the meaning of tokens.", category: "foundations", details: { fullDescription: "Embeddings are dense vector representations that capture the semantic meaning of tokens, words, phrases, or entire documents. Instead of treating words as discrete symbols, embeddings position them in a continuous mathematical space where proximity reflects similarity in meaning.", howItWorks: "Each token is converted into a high-dimensional vector (e.g., 768 or 1536 dimensions). During training, the model adjusts these vectors so that tokens with similar meanings are close in the vector space. Mathematical operations on vectors can reveal semantic relationships: for example, vector('king') - vector('man') + vector('woman') ≈ vector('queen').", examples: ["'cat' and 'feline' will have very similar embeddings in vector space", "1536-dimensional embeddings are used by text-embedding-ada-002", "Semantic search uses cosine similarity between embeddings to find relevant texts"], keyPoints: ["Capture semantic relationships between concepts", "Foundation for semantic search and RAG", "Typical dimensionality: 384 to 4096 dimensions", "Can be pre-computed and stored in vector databases"] } },
                { title: "Latent Space", description: "Mathematical space where embeddings are organized by meaning.", category: "foundations", details: { fullDescription: "Latent Space is the multidimensional mathematical space where the model's internal representations exist. In this space, abstract concepts are organized so that semantic relationships translate into geometric relationships. It is 'latent' because it is not directly observable — it exists only in the model's internal layers.", howItWorks: "As text passes through the model's layers, it is transformed into increasingly abstract representations in this space. Early layers capture syntactic features (grammar, structure), while deeper layers capture semantics (meaning, context, intent). The latent space allows the model to generalize and make analogies.", examples: ["Countries and their capitals form parallel clusters in latent space", "Words in different languages with the same meaning end up close together", "The model can interpolate between concepts to generate creative content"], keyPoints: ["Organizes knowledge in a continuous, navigable way", "Enables generalization to never-seen data", "Latent space quality determines model capability", "Techniques like t-SNE allow visualizing parts of this space in 2D/3D"] } },
                { title: "Parameters", description: "Internal variables that store the patterns learned by the model.", category: "foundations", details: { fullDescription: "Parameters are the numerical values (weights and biases) within the neural network that are adjusted during training to capture patterns in language. The number of parameters is often used as a measure of the model's size and capability — larger models generally demonstrate more sophisticated capabilities.", howItWorks: "Each connection between neurons in the network has an associated weight. During training, these weights are adjusted through backpropagation to minimize prediction error. In a Transformer, parameters include the attention matrices (Query, Key, Value), feed-forward layers, and input/output embeddings.", examples: ["GPT-3 has 175 billion parameters", "LLaMA 2 comes in 7B, 13B, and 70B parameter versions", "Claude and GPT-4 have undisclosed parameter counts"], keyPoints: ["More parameters generally = more capability, but higher cost", "Modern models range from 1B to over 1T parameters", "Quantization can reduce size without significant quality loss", "Scaling laws relate parameters, data, and performance"] } },
                // ── Training ──
                { title: "Pre-training", description: "Training on massive text data to learn language patterns.", category: "training", details: { fullDescription: "Pre-training is the initial and most expensive phase of LLM development, where the model learns the fundamentals of language from enormous amounts of text. During this phase, the model develops a broad understanding of grammar, facts, reasoning, and even some common sense.", howItWorks: "The model is trained with the objective of predicting the next token in text sequences. Billions of texts from the internet, books, articles, and code are processed. Training can take weeks or months on clusters of thousands of GPUs, costing millions of dollars. The result is a base model that can complete text but does not yet follow instructions.", examples: ["GPT-4 training cost an estimated $100 million in computation", "Common Crawl, Wikipedia, and GitHub repositories are common data sources", "LLaMA was trained on 1.4 trillion tokens"], keyPoints: ["The most expensive and time-consuming phase (weeks/months)", "Uses self-supervised learning (next-token prediction)", "Data quality and diversity are crucial", "Defines the model's fundamental capabilities"] } },
                { title: "Base Model", description: "A pre-trained model that predicts text but does not follow instructions.", category: "training", details: { fullDescription: "A Base Model (or Foundation Model) is the direct result of pre-training. It has learned statistical patterns of language and can complete text coherently, but it was not trained to follow instructions or maintain conversations. It simply continues the text it receives in the most probable way.", howItWorks: "When you provide text to a base model, it generates the most statistically probable continuation. If you write a question, instead of answering it, it might generate more similar questions or continue as if it were part of a document. It's like a very sophisticated autocomplete that understands context but not intent.", examples: ["Input: 'The capital of France is' → Probable output: 'Paris, a city known for...'", "Input: 'What is the capital of France?' → May generate: 'What is the capital of Germany?' (completing a quiz)", "GPT-3 base, LLaMA base, and Mistral base are examples of base models"], keyPoints: ["Excellent at completing text, but unpredictable for conversations", "Serves as foundation for fine-tuning and alignment", "Still useful for tasks like few-shot learning", "Researchers often prefer base models for experimentation"] } },
                { title: "Instruct Model", description: "A base model further trained to follow instructions and respond helpfully.", category: "training", details: { fullDescription: "An Instruct Model is a base model that has undergone additional training to understand and follow natural language instructions. This training transforms the model from a simple text completer into an assistant capable of answering questions, following commands, and maintaining productive conversations.", howItWorks: "The process involves supervised fine-tuning with instruction-response pairs created by humans, followed by RLHF (Reinforcement Learning from Human Feedback). Humans write examples of how the model should respond to different types of instructions, and the model learns to replicate this behavioral pattern.", examples: ["GPT-4-turbo, Claude 3 Opus, and Gemini Pro are instruct models", "'Summarize this text in 3 points' → the model understands and executes the task", "ChatGPT is essentially GPT base transformed into an instruct model via RLHF"], keyPoints: ["Follows instructions reliably and predictably", "Maintains coherent and contextual conversations", "Can refuse inappropriate requests (safety training)", "The type of model used in products like ChatGPT and Claude"] } },
                { title: "Fine-Tuning", description: "Additional training on a smaller dataset to shape model behavior.", category: "training", details: { fullDescription: "Fine-tuning is the process of continuing to train a pre-trained model using a smaller, specialized dataset. This allows adapting the model for specific tasks, knowledge domains, or response styles without having to train a model from scratch.", howItWorks: "The base model has its weights slightly adjusted using domain-specific data. Techniques like LoRA (Low-Rank Adaptation) allow efficient fine-tuning by modifying only a small fraction of the parameters. The process typically requires hundreds to thousands of high-quality examples and can be done in hours with a single GPU.", examples: ["Fine-tuning for customer service using real conversation history", "Adapting a model to generate code in a specific language", "LoRA enables fine-tuning of 7B parameter models on a 16GB GPU"], keyPoints: ["Much cheaper and faster than pre-training", "LoRA and QLoRA made fine-tuning accessible", "Risk of catastrophic forgetting (losing previous knowledge)", "Data quality is more important than quantity"] } },
                { title: "Alignment", description: "Ensuring model behavior is helpful, honest, and harmless.", category: "training", details: { fullDescription: "Alignment is the process of ensuring an AI model behaves according to human values and intentions. This includes being helpful, honest, and harmless — the so-called HHH criteria. It is considered one of the biggest challenges in modern AI.", howItWorks: "Alignment combines multiple techniques: supervised fine-tuning with examples of desired behavior, RLHF to optimize for human preferences, Constitutional AI (CAI) where the model learns to follow ethical principles, and red-teaming to identify flaws. The goal is to create models that understand not only WHAT to do, but also WHAT NOT to do.", examples: ["An aligned model refuses to generate instructions for illegal activities", "Anthropic's Constitutional AI uses written principles to guide behavior", "The model admits uncertainty instead of fabricating information (honesty)"], keyPoints: ["Based on HHH criteria: Helpful, Honest, Harmless", "An open problem in AI research", "Includes safety training against malicious use", "Misalignment is an existential risk according to some researchers"] } },
                { title: "RLHF", description: "Using human-ranked responses to guide model behavior.", category: "training", details: { fullDescription: "RLHF (Reinforcement Learning from Human Feedback) is a training technique where human evaluators rank different model responses, and these rankings are used to train a reward model that guides reinforcement learning. It was the key technique that made ChatGPT so effective.", howItWorks: "The process has 3 stages: (1) Supervised fine-tuning with human examples, (2) Training a reward model — humans compare pairs of responses and choose the best one, creating a preference dataset, (3) Optimization via PPO (Proximal Policy Optimization) using the reward model to guide the LLM to generate responses preferred by humans.", examples: ["Evaluators compare two responses and choose the most useful and safe one", "The original ChatGPT extensively used RLHF to improve GPT-3.5", "DPO (Direct Preference Optimization) is a simpler alternative to RLHF"], keyPoints: ["Responsible for ChatGPT's usability revolution", "Requires significant human labor for evaluation", "PPO is the most commonly used RL algorithm in this context", "DPO and RLAIF are more recent and efficient alternatives"] } },
                // ── Prompts ──
                { title: "Prompt", description: "Complete input sent to the model, including instructions and context.", category: "prompting", details: { fullDescription: "A prompt is all the input text sent to the model, including instructions, context, examples, and the question or task itself. The quality of the prompt has a direct impact on the quality of the response — the art and science of creating good prompts is called 'Prompt Engineering'.", howItWorks: "The model processes the entire prompt as context to generate its response. Each token in the prompt influences the probability distribution of output tokens. Well-structured prompts guide the model toward the desired format, style, and content. The order of information, clarity of instructions, and specificity of requirements are crucial factors.", examples: ["Simple prompt: 'What is machine learning?'", "Structured prompt: 'You are a teacher. Explain machine learning to a beginner in 3 paragraphs.'", "Prompt with context: 'Given the article below, extract the 5 main points: [article]'"], keyPoints: ["Prompt Engineering is a valuable and growing skill", "Clear and specific prompts yield better results", "Prompt format affects response format", "Techniques like Chain-of-Thought improve reasoning via prompts"] } },
                { title: "System Prompt", description: "High-level instructions that define the model's role and boundaries.", category: "prompting", details: { fullDescription: "The System Prompt is a special instruction that defines the model's behavior, personality, limitations, and context before user interaction. It functions as a 'configuration' for the assistant, establishing rules that should be followed throughout the conversation.", howItWorks: "The system prompt is processed as the first block of context in the conversation. It has special priority in most implementations and influences all subsequent responses. APIs like OpenAI's and Anthropic's have a dedicated field for system prompts, separate from user messages.", examples: ["'You are a legal assistant specialized in US law. Always cite relevant statutes.'", "'Respond only in JSON. Never include text explanations.'", "'You are a patient tutor. Use simple language and give practical examples.'"], keyPoints: ["Defines the assistant's persona, tone, and limits", "Separate from the user prompt in most APIs", "Can include safety instructions and restrictions", "Not 100% tamper-proof — jailbreaks can circumvent it"] } },
                { title: "User Prompt", description: "Specific question or instruction provided by the user.", category: "prompting", details: { fullDescription: "The User Prompt is the user's direct message to the model — the question, command, or task the user wants the model to execute. It is the most dynamic part of the interaction and varies with each conversation turn.", howItWorks: "The user prompt is combined with the system prompt and conversation history to form the complete context. The model generates its response considering all this information. In multi-turn applications, each new user prompt is added to the history, enabling contextual conversations.", examples: ["'Explain the theory of relativity in simple terms'", "'Fix the errors in this Python code: [code]'", "'Continue the story we started in the previous turn'"], keyPoints: ["The direct interface between human and model", "Combined with system prompt for complete context", "Previous conversation history influences the response", "Clarity and specificity improve response quality"] } },
                { title: "Context Window", description: "Maximum number of tokens the model can process at once.", category: "prompting", details: { fullDescription: "The Context Window is the maximum number of tokens a model can consider simultaneously, including both the input (prompt) and the output (response). It is a fundamental limitation that determines how much information the model can process at once.", howItWorks: "The Transformer's attention mechanism calculates relationships between all pairs of tokens in the window, resulting in quadratic O(n²) complexity. This naturally limits the window size. Techniques like sparse attention, RoPE, and ALiBi allow expanding the window without full quadratic cost.", examples: ["GPT-3.5: 4K-16K tokens | GPT-4: 8K-128K tokens", "Claude 3: up to 200K tokens (~150,000 words or ~500 pages)", "Gemini 1.5 Pro: up to 1M tokens (equivalent to several books)"], keyPoints: ["Includes both input and output in the total limit", "Larger windows = more context but higher cost and latency", "Techniques like RAG help work around context limitations", "The ability to effectively use the full window varies between models"] } },
                { title: "Zero-Shot Learning", description: "Performing a task without examples in the prompt.", category: "prompting", details: { fullDescription: "Zero-Shot Learning is the model's ability to perform a task with only an instruction, without receiving any examples of how the task should be executed. This is possible because the model has internalized enough patterns during pre-training to generalize to new tasks.", howItWorks: "The model uses its knowledge internalized during pre-training to understand the instruction and generate an appropriate response. Zero-shot capability improves significantly with model size — larger models demonstrate zero-shot capabilities that smaller models lack (emergent capabilities).", examples: ["'Classify this text as positive or negative: The movie was amazing!' → 'Positive'", "'Translate to French: Good morning' → 'Bonjour'", "'Summarize this article in 3 points:' — without previous summary examples"], keyPoints: ["Demonstrates the generalization of model knowledge", "Works better in larger, more capable models", "Ideal for common tasks that don't require a specific format", "May be insufficient for highly specialized tasks or rigid formats"] } },
                { title: "Few-Shot Learning", description: "Including examples in the prompt to guide the format or behavior of the output.", category: "prompting", details: { fullDescription: "Few-Shot Learning is the technique of including a few examples (usually 2-5) in the prompt to demonstrate to the model exactly how the task should be executed. The examples serve as a 'template' that the model follows to generate responses in the desired format and style.", howItWorks: "The examples create an in-context pattern that the model recognizes and replicates. There is no weight update — the model simply uses the examples as context for inference. Effectiveness depends on the quality and representativeness of the chosen examples. Diverse and well-chosen examples produce better results.", examples: ["Sentiment: 'I love this product!' → Positive | 'Terrible service' → Negative | 'Fast delivery' → ?", "Providing 3 JSON formatting examples before requesting data conversion", "Showing formal tone examples before requesting a corporate email draft"], keyPoints: ["2-5 examples are usually sufficient", "Diverse examples improve generalization", "Consumes context window tokens (trade-off)", "More effective than zero-shot for tasks with specific formats"] } },
                // ── Inference ──
                { title: "Chain of Thought", description: "Asking the model to show step-by-step reasoning.", category: "inference", details: { fullDescription: "Chain of Thought (CoT) is a prompting technique that instructs the model to 'think aloud', breaking down complex problems into intermediate steps before reaching the final answer. This dramatically improves performance on reasoning, math, and logic tasks.", howItWorks: "By generating intermediate reasoning tokens, the model effectively 'uses' these tokens as additional working memory. Each intermediate step conditions the following steps, enabling more precise reasoning. Variations include zero-shot CoT ('Think step by step') and few-shot CoT (with reasoning examples).", examples: ["'Think step by step: If John has 3 apples and Mary has twice as many, how many do they have together?'", "Model reasons: '1) John has 3 apples. 2) Mary has 3×2=6 apples. 3) Total: 3+6=9'", "Tree of Thought expands CoT with multiple parallel reasoning paths"], keyPoints: ["Significantly improves performance on reasoning tasks", "Just add 'Think step by step' to the prompt (zero-shot CoT)", "Consumes more tokens but produces more reliable answers", "Foundation for reasoning models like o1 and DeepSeek-R1"] } },
                { title: "Inference", description: "The process of generating output tokens from a trained model.", category: "inference", details: { fullDescription: "Inference is the process of using an already trained model to generate responses. Unlike training (which adjusts weights), inference keeps weights fixed and only computes output tokens. It's what happens every time you send a message to ChatGPT or Claude.", howItWorks: "The prompt is tokenized and passed through the model's layers. In the final layer, the model generates a probability distribution over the entire vocabulary for the next token. A token is selected (using temperature and other parameters), added to the sequence, and the process repeats autoregressively until generating a stop token or reaching the limit.", examples: ["Every message sent to ChatGPT initiates an inference process", "Batch inference: processing multiple requests simultaneously for efficiency", "Speculative decoding uses a smaller model to accelerate large model inference"], keyPoints: ["Autoregressive process: one token at a time, sequentially", "Much cheaper than training, but still significant at scale", "KV-cache optimizes re-computations during generation", "Techniques like quantization reduce inference costs"] } },
                { title: "Latency", description: "Time between sending a prompt and receiving the output.", category: "inference", details: { fullDescription: "Latency is the model's response time, measured from sending the prompt to receiving the complete response (or the first token, in the case of streaming). It is a critical factor for user experience and real-time applications.", howItWorks: "Total latency includes: network time, input tokenization, prompt processing (prefill), and autoregressive generation of each token (decode). Prefill processes all input tokens in parallel and is proportional to prompt size. Decode is sequential and proportional to response size. Streaming shows tokens as they are generated, reducing perceived latency.", examples: ["Time to First Token (TTFT): typically 0.5-2 seconds for large models", "Streaming reduces perceived latency by showing tokens as they're generated", "Smaller models (7B) can have TTFT under 100ms on adequate hardware"], keyPoints: ["TTFT (Time to First Token) is the most important UX metric", "Larger prompts increase prefill latency", "Smaller models are significantly faster", "Edge deployment and quantization help reduce latency"] } },
                { title: "Temperature", description: "A parameter that controls randomness in token selection.", category: "inference", details: { fullDescription: "Temperature is a hyperparameter that controls randomness/creativity in text generation. Low values (0-0.3) make the model more deterministic and focused, while high values (0.7-1.5) make responses more diverse and creative, but potentially less coherent.", howItWorks: "Temperature modifies the probability distribution of tokens before sampling. Mathematically, logits (raw scores) are divided by the temperature before softmax. Temperature 0 always selects the most probable token (greedy). Temperature 1 maintains the original distribution. Temperature > 1 'flattens' the distribution, giving less probable tokens more chance.", examples: ["Temperature 0: 'The capital of France is Paris.' (always the same)", "Temperature 0.7: varied but coherent responses — ideal for creative writing", "Temperature 1.5: may generate surprising but sometimes incoherent text"], keyPoints: ["0 = deterministic, 1 = original distribution, >1 = more random", "Use low (0-0.3) for factual tasks and code", "Use medium (0.5-0.8) for balanced creative writing", "Top-p (nucleus sampling) is another complementary parameter"] } },
                { title: "Hallucination", description: "Confident generation of incorrect or fabricated information.", category: "inference", details: { fullDescription: "Hallucination occurs when the model generates information that appears correct and is presented with confidence, but is factually incorrect, invented, or unfounded. It is one of the biggest challenges of current LLMs, as the model doesn't distinguish between 'remembering' real facts and 'inventing' plausible ones.", howItWorks: "LLMs are trained to generate statistically plausible text, not necessarily truthful text. The model learns patterns of how facts are expressed, but doesn't have a verifiable 'database'. When the model doesn't 'know' something, it may generate a response that follows the correct statistical pattern but contains fabricated information. This occurs because the model optimizes for plausibility, not veracity.", examples: ["Citing academic papers with completely invented titles, authors, and DOIs", "Confidently stating a historical event occurred on an incorrect date", "Inventing non-existent features of programming libraries"], keyPoints: ["The model doesn't know it's 'inventing' — it generates plausible text", "Techniques like RAG and Grounding help mitigate hallucinations", "Human verification remains essential for critical content", "More recent models hallucinate less, but the problem persists"] } },
                { title: "Grounding", description: "Constraining outputs to provided or verifiable information.", category: "inference", details: { fullDescription: "Grounding is the process of anchoring model responses to verifiable and reliable sources of information, rather than relying solely on knowledge internalized during training. It is the main strategy for combating hallucinations and ensuring factual accuracy.", howItWorks: "The model receives documents, data, or verifiable context along with the instruction, and is directed to base its responses exclusively on those sources. Techniques include RAG (fetching relevant information before responding), source citation, and explicit instructions like 'respond only based on the provided context'.", examples: ["'Based ONLY on the document below, answer:' — explicit grounding", "RAG fetches relevant documents and includes them in the prompt as context", "Google Search Grounding: model queries the web before responding"], keyPoints: ["Primary defense against hallucinations", "RAG is the most popular grounding technique", "Can include citations and references for verification", "Trade-off: limits creativity but increases reliability"] } },
                // ── Application ──
                { title: "RAG", description: "Retrieving external data and adding it to the prompt before generation.", category: "application", details: { fullDescription: "RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. Instead of relying solely on the model's knowledge, RAG first searches for relevant documents in a database and includes them in the prompt, enabling up-to-date, accurate, and citable responses.", howItWorks: "The process has 3 stages: (1) Indexing: documents are converted into embeddings and stored in a vector database. (2) Retrieval: the user's question is converted into an embedding and the most similar documents are retrieved via similarity search. (3) Generation: retrieved documents are included in the prompt along with the question, and the LLM generates a response based on this context.", examples: ["Corporate chatbot that searches internal manuals before responding", "Legal assistant that consults updated legislation via RAG", "Tools like Perplexity AI use RAG with real-time web search"], keyPoints: ["Solves the problem of outdated model knowledge", "Reduces hallucinations by providing verifiable sources", "Vector databases like Pinecone, Weaviate, and ChromaDB are used", "Retrieval quality determines response quality"] } },
                { title: "Workflow", description: "A fixed, predefined sequence where the LLM follows established steps.", category: "application", details: { fullDescription: "An LLM Workflow is a predefined, deterministic sequence of steps where the model is used as a component at specific points. Unlike an autonomous agent, the execution flow is fixed and controlled by code, not by the model. Each step has defined inputs and outputs.", howItWorks: "The developer defines a pipeline with sequential or parallel steps. At each step, the LLM may be called for a specific task (classify, extract, generate, etc.). Control logic (conditionals, loops, routing) is implemented in code, not delegated to the model. Frameworks like LangChain and LlamaIndex facilitate workflow creation.", examples: ["Email analysis pipeline: classify → extract entities → generate response → review", "Report generation: collect data → analyze → generate text → format", "Document processing: OCR → chunking → embedding → indexing"], keyPoints: ["Predictable and debuggable flow (vs. autonomous agents)", "Each step can use a different optimized prompt", "Easier to test and monitor than agents", "Ideal when the sequence of steps is known in advance"] } },
                { title: "Agent", description: "A system where the LLM plans actions, then dynamically chooses steps and tools.", category: "application", details: { fullDescription: "An AI Agent is a system where the LLM acts as the 'brain' that autonomously plans, makes decisions, and executes actions using external tools. Unlike fixed workflows, agents dynamically determine which steps to take based on context and intermediate results.", howItWorks: "The agent follows a loop: (1) Observe the current state and objective, (2) Reason about the next step (using CoT), (3) Select and execute a tool (search, code, API, etc.), (4) Observe the result, (5) Decide whether the objective has been achieved or more actions are needed. Frameworks like AutoGPT, CrewAI, and Claude with tool use implement this pattern.", examples: ["Claude Code: agent that reads code, plans changes, and executes commands", "Research agent that searches the web, reads articles, and synthesizes information", "Customer service agent that queries databases, processes orders, and escalates to humans"], keyPoints: ["Autonomy: dynamically decides which actions to take", "Uses tools (tool use) to interact with the world", "More flexible than workflows, but less predictable", "Requires robust guardrails to prevent undesired actions"] } },
                { title: "Multimodality", description: "Ability to process multiple input types, such as text and images.", category: "application", details: { fullDescription: "Multimodality is the ability of a model to process and/or generate different types of data — text, images, audio, video, and code. Multimodal models can understand context through multiple modalities simultaneously, approaching human perception.", howItWorks: "Multimodal models use specialized encoders for each input type (e.g., Vision Transformer for images) that convert data from different modalities into the same embedding space. The model can then reason about all modalities simultaneously. Some models are multimodal in input (understand images) but unimodal in output (generate only text).", examples: ["GPT-4V and Claude 3 can analyze images and answer questions about them", "Gemini can process text, images, audio, and video simultaneously", "DALL-E and Midjourney generate images from text descriptions"], keyPoints: ["Text + image is the most common multimodal combination", "Recent models are adding audio and video", "Enables applications like document analysis, smart OCR, and accessibility", "Quality varies significantly across modalities and models"] } },
                { title: "Benchmarks", description: "Standardized tests used to compare model capabilities.", category: "application", details: { fullDescription: "Benchmarks are standardized test suites used to measure and compare the performance of different AI models on specific tasks. They include reasoning questions, math, programming, general knowledge, and more, providing objective comparison metrics.", howItWorks: "Each benchmark consists of a set of problems with verifiable answers. Models are evaluated without access to answers, and their score is calculated based on accuracy. Benchmarks are carefully designed to test specific capabilities and minimize the possibility of memorization (data contamination).", examples: ["MMLU: 57 academic subjects — tests broad general knowledge", "HumanEval: programming problems — tests coding ability", "GSM8K: grade-school math problems — tests quantitative reasoning"], keyPoints: ["Essential for objective comparison between models", "Data contamination is a risk (model memorizes benchmark questions)", "No benchmark fully captures a model's real-world usefulness", "Leaderboards like LMSYS Chatbot Arena use human evaluation"] } },
                { title: "Guardrails", description: "Systems that block unsafe or inappropriate inputs and outputs.", category: "application", details: { fullDescription: "Guardrails are safety systems that filter, validate, and control the inputs and outputs of LLMs to prevent misuse, harmful content, and undesired behaviors. They function as protective layers around the model, ensuring it operates within acceptable boundaries.", howItWorks: "Guardrails operate in multiple layers: (1) Input filters — detect and block malicious prompts, jailbreaks, and prohibited content before reaching the model. (2) Output rules — check the model's response against defined policies (PII, toxicity, forbidden topics). (3) Structural validation — ensure the output is in the correct format (JSON, length, etc.).", examples: ["Blocking generation of malicious code or dangerous instructions", "Detecting and redacting personal information (PII) in responses", "NVIDIA's NeMo Guardrails allows defining declarative conversational rules"], keyPoints: ["Essential for production deployment of LLM applications", "Include protection against prompt injection and jailbreaks", "Frameworks like Guardrails AI and NeMo facilitate implementation", "Must be continuously tested with red-teaming"] } },
            ],
        };

        // ── Flag SVG components ──
        const BrazilFlag = () => (
            <svg viewBox="0 0 640 480" className="w-8 h-6 rounded-sm">
                <rect width="640" height="480" fill="#009b3a"/>
                <polygon points="320,39 609,240 320,441 31,240" fill="#fedf00"/>
                <circle cx="320" cy="240" r="95" fill="#002776"/>
                <path d="M196,240 Q320,160 444,240 Q320,200 196,240" fill="#fff"/>
            </svg>
        );

        const UKFlag = () => (
            <svg viewBox="0 0 640 480" className="w-8 h-6 rounded-sm">
                <rect width="640" height="480" fill="#012169"/>
                <path d="M0,0 L640,480 M640,0 L0,480" stroke="#fff" strokeWidth="80"/>
                <path d="M0,0 L640,480 M640,0 L0,480" stroke="#C8102E" strokeWidth="50" clipPath="url(#ukclip)"/>
                <clipPath id="ukclip"><path d="M320,0 L640,0 640,240 320,240Z M320,240 L0,240 0,480 320,480Z"/></clipPath>
                <path d="M320,0 V480 M0,240 H640" stroke="#fff" strokeWidth="100"/>
                <path d="M320,0 V480 M0,240 H640" stroke="#C8102E" strokeWidth="60"/>
            </svg>
        );

        // ── Modal Component ──
        const DetailModal = ({ concept, onClose, t }) => {
            const cat = categoryConfig[concept.category];
            const details = concept.details;

            useEffect(() => {
                const handleEsc = (e) => { if (e.key === 'Escape') onClose(); };
                document.addEventListener('keydown', handleEsc);
                document.body.style.overflow = 'hidden';
                return () => {
                    document.removeEventListener('keydown', handleEsc);
                    document.body.style.overflow = '';
                };
            }, [onClose]);

            return (
                <div className="fixed inset-0 z-50 flex items-center justify-center p-4 modal-overlay" onClick={onClose}>
                    <div className="absolute inset-0 bg-black/80 backdrop-blur-sm"></div>
                    <div className="relative w-full max-w-2xl max-h-[90vh] rounded-2xl border overflow-hidden modal-content"
                        style={{ borderColor: cat.neonColor + '40', boxShadow: `0 0 40px ${cat.neonColor}20, 0 0 80px ${cat.neonColor}10` }}
                        onClick={(e) => e.stopPropagation()}>
                        <div className="bg-darkSurface px-6 py-5 border-b flex items-center justify-between" style={{ borderColor: cat.neonColor + '30' }}>
                            <div className="flex items-center gap-3">
                                <div className="w-3 h-3 rounded-full" style={{ backgroundColor: cat.neonColor, boxShadow: `0 0 10px ${cat.neonColor}` }}></div>
                                <h2 className="text-2xl font-bold" style={{ color: cat.neonColor, textShadow: `0 0 20px ${cat.neonColor}60` }}>{concept.title}</h2>
                                <span className="text-xs px-2 py-1 rounded-full bg-white/5 text-gray-400 border border-white/10">{t.categories[concept.category]}</span>
                            </div>
                            <button onClick={onClose} className="p-2 rounded-lg hover:bg-white/10 transition-colors text-gray-400 hover:text-white"><X size={20} /></button>
                        </div>
                        <div className="bg-darkCard overflow-y-auto modal-scroll" style={{ maxHeight: 'calc(90vh - 80px)' }}>
                            <div className="p-6 space-y-6">
                                <div><p className="text-gray-300 leading-relaxed text-[15px]">{details.fullDescription}</p></div>
                                <div className="rounded-xl p-5 border bg-white/[0.02]" style={{ borderColor: cat.neonColor + '20' }}>
                                    <div className="flex items-center gap-2 mb-3">
                                        <Lightbulb size={18} style={{ color: cat.neonColor }} />
                                        <h3 className="font-semibold text-white">{t.howItWorks}</h3>
                                    </div>
                                    <p className="text-gray-400 leading-relaxed text-sm">{details.howItWorks}</p>
                                </div>
                                <div>
                                    <div className="flex items-center gap-2 mb-3">
                                        <Code size={18} style={{ color: cat.neonColor }} />
                                        <h3 className="font-semibold text-white">{t.examples}</h3>
                                    </div>
                                    <div className="space-y-2">
                                        {details.examples.map((ex, i) => (
                                            <div key={i} className="flex items-start gap-3 text-sm">
                                                <ArrowRight size={14} className="mt-1 flex-shrink-0" style={{ color: cat.neonColor }} />
                                                <span className="text-gray-400">{ex}</span>
                                            </div>
                                        ))}
                                    </div>
                                </div>
                                <div className="rounded-xl p-5 border bg-white/[0.02]" style={{ borderColor: cat.neonColor + '20' }}>
                                    <div className="flex items-center gap-2 mb-3">
                                        <BookOpen size={18} style={{ color: cat.neonColor }} />
                                        <h3 className="font-semibold text-white">{t.keyPoints}</h3>
                                    </div>
                                    <div className="grid grid-cols-1 sm:grid-cols-2 gap-2">
                                        {details.keyPoints.map((point, i) => (
                                            <div key={i} className="flex items-start gap-2 text-sm">
                                                <span className="mt-1.5 w-1.5 h-1.5 rounded-full flex-shrink-0" style={{ backgroundColor: cat.neonColor }}></span>
                                                <span className="text-gray-400">{point}</span>
                                            </div>
                                        ))}
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            );
        };

        // ── FlipCard Component ──
        const FlipCard = ({ item, onShowDetails, t }) => {
            const [isFlipped, setIsFlipped] = useState(false);
            const cat = categoryConfig[item.category];

            const handleDetailsClick = (e) => { e.stopPropagation(); onShowDetails(item); };

            return (
                <div className="group h-52 w-full cursor-pointer [perspective:1000px]" onClick={() => setIsFlipped(!isFlipped)}>
                    <div className={`relative h-full w-full transition-all duration-500 [transform-style:preserve-3d] rounded-2xl ${isFlipped ? '[transform:rotateY(180deg)]' : ''}`}>
                        <div className={`absolute inset-0 flex flex-col items-center justify-center p-4 text-center rounded-2xl border [backface-visibility:hidden] bg-darkCard ${cat.borderColor} ${cat.glowClass}`}>
                            <div className="mb-2 opacity-40"><Sparkles size={20} style={{ color: cat.neonColor }} /></div>
                            <h3 className={`text-xl font-bold tracking-tight ${cat.textClass}`}>{item.title}</h3>
                            <span className="mt-1 text-[10px] px-2 py-0.5 rounded-full bg-white/5 text-gray-500 border border-white/10">{t.categories[item.category]}</span>
                            <p className="mt-4 text-[10px] font-semibold uppercase text-gray-600">{t.tapToFlip}</p>
                        </div>
                        <div className={`absolute inset-0 flex flex-col items-center justify-center p-6 text-center rounded-2xl border bg-darkSurface ${cat.borderColor} [transform:rotateY(180deg)] [backface-visibility:hidden] ${cat.glowClass}`}>
                            <p className="text-sm font-medium leading-relaxed text-gray-300 mb-4">{item.description}</p>
                            <button onClick={handleDetailsClick}
                                className="flex items-center gap-1.5 text-xs font-semibold px-4 py-2 rounded-full border transition-all duration-200 hover:scale-105"
                                style={{ color: cat.neonColor, borderColor: cat.neonColor + '50', backgroundColor: cat.neonColor + '15', boxShadow: `0 0 15px ${cat.neonColor}20` }}>
                                <ExternalLink size={12} />{t.moreDetails}
                            </button>
                        </div>
                    </div>
                </div>
            );
        };

        // ── App ──
        function App() {
            const [lang, setLang] = useState('pt');
            const [searchTerm, setSearchTerm] = useState('');
            const [selectedConcept, setSelectedConcept] = useState(null);

            const t = ui[lang];
            const data = concepts[lang];

            const filteredConcepts = data.filter(c =>
                c.title.toLowerCase().includes(searchTerm.toLowerCase()) ||
                c.description.toLowerCase().includes(searchTerm.toLowerCase())
            );

            // Update html lang attribute
            useEffect(() => {
                document.documentElement.lang = lang === 'pt' ? 'pt-BR' : 'en';
                document.title = t.pageTitle;
            }, [lang]);

            return (
                <div className="min-h-screen bg-darkBg text-gray-100 font-sans pb-12">
                    {/* Header */}
                    <header className="bg-darkSurface/80 backdrop-blur-md border-b border-white/10 sticky top-0 z-10">
                        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-4">
                            <div className="flex flex-col md:flex-row items-center justify-between gap-4 flex-wrap">
                                <div className="flex items-center gap-3">
                                    <div className="p-2 rounded-lg border border-neonCyan/30 neon-glow-cyan">
                                        <Brain size={24} className="text-neonCyan" />
                                    </div>
                                    <h1 className="text-2xl font-bold neon-text-cyan">{t.pageTitle}</h1>
                                </div>

                                <div className="flex items-center gap-3">
                                    {/* Language Selector */}
                                    <div className="flex items-center gap-1.5 bg-white/5 border border-white/10 rounded-full px-2 py-1.5">
                                        <button onClick={() => { setLang('pt'); setSearchTerm(''); setSelectedConcept(null); }}
                                            className={`flag-btn rounded-sm overflow-hidden ${lang === 'pt' ? 'active' : ''}`}
                                            title="Português">
                                            <BrazilFlag />
                                        </button>
                                        <div className="w-px h-4 bg-white/10"></div>
                                        <button onClick={() => { setLang('en'); setSearchTerm(''); setSelectedConcept(null); }}
                                            className={`flag-btn rounded-sm overflow-hidden ${lang === 'en' ? 'active' : ''}`}
                                            title="English">
                                            <UKFlag />
                                        </button>
                                    </div>

                                    <a href="https://github.com/andrecorumba/llm-concepts" target="_blank" rel="noopener noreferrer"
                                        className="text-gray-500 hover:text-neonCyan transition-colors" title="GitHub">
                                        <Github size={22} />
                                    </a>
                                </div>

                                <div className="relative w-full md:w-96">
                                    <div className="absolute inset-y-0 left-0 pl-3 flex items-center pointer-events-none text-gray-500">
                                        <Search size={18} />
                                    </div>
                                    <input type="text" placeholder={t.searchPlaceholder}
                                        className="block w-full pl-10 pr-3 py-2 border border-white/10 rounded-full leading-5 bg-white/5 placeholder-gray-600 text-gray-200 focus:outline-none focus:bg-white/10 focus:ring-2 focus:ring-neonCyan/50 focus:border-neonCyan/30 transition duration-150 ease-in-out sm:text-sm"
                                        value={searchTerm} onChange={(e) => setSearchTerm(e.target.value)}
                                    />
                                </div>
                            </div>
                        </div>
                    </header>

                    <main className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
                        <div className="mb-8 text-center max-w-2xl mx-auto">
                            <h2 className="text-lg text-gray-400">{t.subtitle}</h2>
                            <p className="text-sm text-gray-600 mt-2">{t.tapHint}</p>
                        </div>

                        <div className="flex flex-wrap justify-center gap-4 mb-8 text-xs font-medium text-gray-500">
                            {Object.entries(categoryConfig).map(([key, cat]) => (
                                <span key={key} className="flex items-center gap-1.5">
                                    <span className={`w-2.5 h-2.5 rounded-full ${cat.dotColor}`} style={{ boxShadow: `0 0 6px ${cat.neonColor}80` }}></span>
                                    {t.categories[key]}
                                </span>
                            ))}
                        </div>

                        {filteredConcepts.length > 0 ? (
                            <div className="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-6">
                                {filteredConcepts.map((item, index) => (
                                    <FlipCard key={index} item={item} onShowDetails={setSelectedConcept} t={t} />
                                ))}
                            </div>
                        ) : (
                            <div className="text-center py-20 text-gray-600">
                                <Search size={48} className="mx-auto mb-4 opacity-20" />
                                <p className="text-lg">{t.noResults(searchTerm)}</p>
                                <button onClick={() => setSearchTerm('')}
                                    className="mt-4 text-neonCyan hover:text-neonCyan/80 font-medium flex items-center gap-2 mx-auto">
                                    <RefreshCw size={16} /> {t.clearSearch}
                                </button>
                            </div>
                        )}
                    </main>

                    <footer className="max-w-7xl mx-auto px-4 py-6 text-center text-gray-600 text-sm border-t border-white/5 mt-8">
                        <p>{t.footer}</p>
                    </footer>

                    {selectedConcept && (
                        <DetailModal concept={selectedConcept} onClose={() => setSelectedConcept(null)} t={t} />
                    )}
                </div>
            );
        }

        const rootElement = document.getElementById('root');
        if (rootElement) {
            const root = createRoot(rootElement);
            root.render(<App />);
        }
    </script>
</body>

</html>
